# sanit{AI}ze

A Web application firewall that protects AI agents and LLM applications from prompt injection attacks and malicious inputs.

## Features

- ğŸ›¡ï¸ **Input Filtering**: Detect prompt injections, jailbreaks, and malicious user inputs
- ğŸ” **Output Filtering**: Validate LLM responses for policy violations and harmful content
- ğŸ¯ **Flexible Configuration**: Customizable risk tolerance and policy enforcement
- ğŸ”Œ **Provider Agnostic**: Works with any OpenAI-compatible API
- ğŸ“¦ **Easy Integration**: Simple singleton API with TypeScript support

## Quick Start

```typescript
import { sanitaize } from "sanitaize";

sanitaize.config = {
	app_policy: "No harmful or illegal content",
	risk_tolerance: "medium",
	firewall_model: "gpt-4",
	firewall_api_key: process.env.OPENAI_API_KEY!,
};

const verdict = await sanitaize.judgeInput(userPrompt);
if (verdict.verdict === "block") {
	console.log("Attack detected:", verdict.attack_types);
}
```

## Setup

See [SETUP.md](./SETUP.md) for detailed setup instructions.

## Project Structure

```
sanit-AI-ze/
â”œâ”€â”€ packages/sanitaize/    # Core NPM package
â””â”€â”€ apps/sanitaize/        # Demo web application
```
